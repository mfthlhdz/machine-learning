{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Jz2DjS99uo"
      },
      "source": [
        "# Pertemuan 14 - Arsitektur Model Machine Learning\n",
        "\n",
        "## Capaian Pembelajaran\n",
        "> Mampu merancang arsitektur model machine learning yang paling sesuai dengan kebutuhan spesifik, termasuk pemilihan fitur, algoritma, dan parameter yang optimal.\n",
        "\n",
        "## Pokok Bahasan\n",
        "1. Arsitektur Machine Learning\n",
        "2. Teknik Pemilihan Fitur\n",
        "3. Teknik Pemilihan Algoritma\n",
        "4. Teknik Evaluasi\n",
        "5. Optimasi Parameter\n",
        "6. Dasar Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NX1Oppy99uu"
      },
      "source": [
        "## 1. Arsitektur Machine Learning\n",
        "Arsitektur machine learning mencakup desain dan struktur sistem yang mengelola aliran data, pemrosesan, dan prediksi dari model machine learning. Berikut adalah penjelasan tentang arsitektur machine learning secara umum:\n",
        "\n",
        "#### 1.1. Pengumpulan Data\n",
        "Proses ini melibatkan pengumpulan data dari berbagai sumber, seperti database, file, API, atau streaming data. Data ini harus relevan dengan masalah yang ingin diselesaikan dan berkualitas tinggi.\n",
        "\n",
        "#### 1.2. Pra-Pemrosesan Data\n",
        "Data yang dikumpulkan biasanya memerlukan pra-pemrosesan sebelum dapat digunakan untuk pelatihan model. Langkah-langkah pra-pemrosesan meliputi:\n",
        "- Pembersihan Data: Menghapus atau memperbaiki data yang hilang, duplikat, atau tidak konsisten.\n",
        "- Transformasi Data: Normalisasi atau standarisasi fitur untuk memastikan skala data konsisten.\n",
        "- Pengkodean Data Kategorikal: Mengonversi fitur kategorikal menjadi format numerik.\n",
        "- Pembagian Data: Membagi data menjadi subset pelatihan, validasi, dan pengujian.\n",
        "\n",
        "#### 1.3. Pemilihan Model\n",
        "Memilih algoritma machine learning yang sesuai berdasarkan jenis masalah (klasifikasi, regresi, clustering, dll.) dan kriteria pemilihan model (kinerja, kecepatan, interpretabilitas, dll.).\n",
        "\n",
        "#### 1.4. Pelatihan Model\n",
        "Melatih model dengan menggunakan data pelatihan. Selama pelatihan, model belajar untuk mengidentifikasi pola dalam data. Proses ini melibatkan:\n",
        "- Optimasi: Menggunakan algoritma optimisasi seperti Gradient Descent untuk memperbarui parameter model.\n",
        "- Validasi: Menggunakan data validasi untuk menghindari overfitting dan memilih hyperparameter terbaik.\n",
        "\n",
        "#### 1.5. Evaluasi Model\n",
        "Setelah pelatihan, model diuji menggunakan data pengujian untuk mengevaluasi kinerjanya. Metrik evaluasi yang umum termasuk akurasi, precision, recall, F1-score, dan AUC-ROC untuk masalah klasifikasi, serta mean squared error (MSE) dan RÂ² untuk regresi.\n",
        "\n",
        "#### 1.6. Tuning Hyperparameter\n",
        "Menyesuaikan hyperparameter model untuk meningkatkan kinerja. Ini sering dilakukan dengan teknik seperti grid search atau random search.\n",
        "\n",
        "#### 1.7. Deploy dan Integrasi\n",
        "Mengimplementasikan model ke lingkungan produksi, di mana model dapat digunakan untuk prediksi dengan data baru. Ini melibatkan integrasi model dengan sistem yang ada, serta pengaturan pipeline otomatis untuk inferensi.\n",
        "\n",
        "#### 1.8. Monitoring dan Pemeliharaan\n",
        "Memantau kinerja model di lingkungan produksi dan melakukan pembaruan atau retrain model secara berkala untuk memastikan bahwa model tetap relevan dan akurat.\n",
        "\n",
        "___\n",
        "\n",
        "##### Arsitektur Machine Learning Berbasis Deep Learning\n",
        "##### Arsitektur Jaringan Saraf\n",
        "- Jaringan Saraf Feedforward: Termasuk layer input, hidden layers, dan output layer. Data mengalir dalam satu arah dari input ke output.\n",
        "- Jaringan Saraf Konvolusi (CNN): Digunakan terutama untuk pengolahan citra dan video, mengintegrasikan layer konvolusi, pooling, dan fully connected layers.\n",
        "- Jaringan Saraf Rekuren (RNN): Digunakan untuk data sekuensial, seperti teks dan time series, dengan kemampuan untuk menangkap ketergantungan temporal.\n",
        "- Transformers: Arsitektur yang populer untuk pemrosesan bahasa alami (NLP), menggunakan mekanisme perhatian (attention) untuk menangkap hubungan antar kata dalam kalimat.\n",
        "\n",
        "##### Pipeline Deep Learning\n",
        "- Preprocessing: Melakukan transformasi data khusus untuk deep learning, seperti augmentasi citra.\n",
        "- Training: Menggunakan teknik khusus seperti backpropagation untuk melatih jaringan saraf dengan data besar.\n",
        "- Evaluation: Metrik evaluasi khusus untuk deep learning, termasuk loss function yang relevan dengan task (cross-entropy untuk klasifikasi, MSE untuk regresi).\n",
        "- Deployment: Mengintegrasikan model deep learning ke dalam aplikasi dengan efisiensi dan skalabilitas.\n",
        "\n",
        "##### Arsitektur Machine Learning dalam Skala Besar\n",
        "- Data Pipeline: Mengelola aliran data besar dengan menggunakan alat dan platform seperti Apache Kafka, Apache Spark, dan Google Dataflow.\n",
        "- Training dan Inferensi di Cloud: Menggunakan layanan cloud seperti AWS SageMaker, Google AI Platform, dan Azure Machine Learning untuk pelatihan dan inferensi skala besar dengan dukungan GPU/TPU.\n",
        "- Model Management: Mengelola versi model, metadata, dan artefak model menggunakan platform seperti MLflow, DVC, atau ModelDB.\n",
        "- Scalability dan Distributed Training: Memanfaatkan teknik pelatihan terdistribusi dan parallel untuk mempercepat pelatihan model pada dataset besar dan infrastruktur komputasi besar.\n",
        "\n",
        "Arsitektur machine learning yang efektif mengintegrasikan semua elemen ini, dari pengumpulan data hingga deployment dan pemeliharaan, untuk membangun sistem yang kuat dan skalabel yang dapat memberikan wawasan dan prediksi yang berharga.\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrXfVJ0299uw"
      },
      "source": [
        "## 2. Teknik Pemilihan Fitur\n",
        "Pemilihan fitur (feature selection) adalah proses penting dalam machine learning yang bertujuan untuk memilih subset fitur yang paling relevan dari data untuk digunakan dalam membangun model. Teknik pemilihan fitur membantu meningkatkan kinerja model, mengurangi overfitting, dan mengurangi waktu pelatihan. Berikut adalah berbagai teknik pemilihan fitur yang sering digunakan:\n",
        "\n",
        "\n",
        "#### a. Filter Methods\n",
        "Filter methods menilai fitur secara individual dan memilih fitur berdasarkan metrik statistik. Teknik ini tidak melibatkan model pembelajaran mesin.\n",
        "- Chi-Square Test: Mengukur ketergantungan antara fitur dan label target. Fitur yang memiliki nilai chi-square tinggi dianggap lebih relevan.\n",
        "- Mutual Information: Mengukur seberapa besar ketergantungan informasi antara fitur dan label target. Fitur dengan mutual information tinggi dianggap lebih informatif.\n",
        "- Correlation Coefficient: Mengukur kekuatan hubungan linier antara fitur dan label target. Fitur dengan korelasi tinggi dianggap lebih penting.\n",
        "- ANOVA F-Value: Untuk fitur numerik dengan label kategorikal, ANOVA F-test mengukur variasi antar kelompok dibandingkan dengan variasi dalam kelompok.\n",
        "\n",
        "\n",
        "#### b. Wrapper Methods\n",
        "Wrapper methods menggunakan model pembelajaran mesin untuk menilai subset fitur. Ini melibatkan pelatihan model berulang kali dengan berbagai subset fitur.\n",
        "- Forward Selection: Memulai dengan fitur kosong dan menambahkan fitur satu per satu yang paling meningkatkan kinerja model. Proses ini berhenti ketika penambahan fitur tidak meningkatkan kinerja secara signifikan.\n",
        "- Backward Elimination: Memulai dengan semua fitur dan secara iteratif menghapus fitur yang paling sedikit mempengaruhi kinerja model. Proses ini berhenti ketika menghapus fitur mulai menurunkan kinerja model.\n",
        "- Recursive Feature Elimination (RFE): Menggunakan model pembelajaran mesin untuk menghapus fitur yang kurang penting secara iteratif hingga mencapai jumlah fitur yang diinginkan.\n",
        "\n",
        "\n",
        "#### c. Embedded Methods\n",
        "Embedded methods mengintegrasikan pemilihan fitur sebagai bagian dari proses pelatihan model. Teknik ini sering melibatkan model yang secara otomatis memilih fitur yang relevan selama pelatihan.\n",
        "- Lasso Regression (L1 Regularization): Menggunakan regularisasi L1 untuk memaksa beberapa koefisien fitur menjadi nol, sehingga secara otomatis memilih fitur yang lebih relevan.\n",
        "- Decision Trees and Random Forests: Menggunakan pentingnya fitur yang dihitung berdasarkan bagaimana fitur membagi data pada node dalam pohon keputusan. Fitur yang sering digunakan untuk membuat pembagian signifikan dianggap lebih penting.\n",
        "- Gradient Boosting Machines (GBM): Mengukur pentingnya fitur berdasarkan kontribusi fitur terhadap pengurangan kesalahan model dalam proses boosting.\n",
        "\n",
        "\n",
        "#### Dimensionality Reduction\n",
        "Teknik ini mengurangi jumlah fitur dengan mentransformasi data ke dalam dimensi yang lebih rendah, biasanya dengan menjaga informasi sebanyak mungkin.\n",
        "- Principal Component Analysis (PCA): Mengurangi dimensi data dengan mentransformasikan fitur asli ke dalam komponen utama yang memiliki variansi terbesar. PCA menghasilkan fitur baru yang merupakan kombinasi linier dari fitur asli.\n",
        "- Linear Discriminant Analysis (LDA): Mengurangi dimensi dengan memaksimalkan pemisahan antar kelas dalam data. LDA mencari kombinasi fitur yang memaksimalkan variansi antar kelas dan meminimalkan variansi dalam kelas.\n",
        "- t-Distributed Stochastic Neighbor Embedding (t-SNE): Mengurangi dimensi data dengan mempertahankan struktur lokal data. Ini berguna untuk visualisasi data dengan dimensi tinggi.\n",
        "\n",
        "#### Feature Engineering\n",
        "Feature engineering melibatkan penciptaan fitur baru yang dapat lebih baik mewakili informasi dalam data.\n",
        "- Interaction Terms: Membuat fitur baru yang merupakan hasil perkalian dari fitur yang ada untuk menangkap hubungan interaksi antara fitur.\n",
        "- Polynomial Features: Membuat fitur baru dengan mengkuadratkan atau mengalikan fitur yang ada untuk menangkap hubungan non-linier.\n",
        "- Binning: Mengelompokkan nilai fitur kontinu ke dalam bin diskret untuk menangkap informasi yang terlewatkan oleh model.\n",
        "\n",
        "\n",
        "#### Domain Knowledge\n",
        "Memanfaatkan pengetahuan domain untuk memilih fitur yang dianggap relevan berdasarkan pemahaman mendalam tentang masalah yang sedang dipecahkan.\n",
        "- Expert Input: Mengandalkan ahli domain untuk memberikan wawasan tentang fitur mana yang dianggap penting.\n",
        "- Manual Selection: Memilih fitur berdasarkan pengetahuan dan pengalaman dalam bidang yang relevan.\n",
        "\n",
        "Teknik pemilihan fitur yang tepat bergantung pada jenis data, tujuan model, dan algoritma yang digunakan. Kombinasi dari beberapa teknik sering kali memberikan hasil terbaik dalam mengidentifikasi fitur yang paling relevan dan meningkatkan kinerja model machine learning.\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVRz9yQC99uw"
      },
      "source": [
        "## 3. Teknik Pemilihan Algoritma\n",
        "Pemilihan algoritma dalam machine learning memerlukan pertimbangan yang lebih spesifik dibandingkan pemilihan algoritma pada umumnya. Berikut adalah beberapa teknik dan faktor yang perlu diperhatikan dalam pemilihan algoritma machine learning:\n",
        "\n",
        "#### a. Tipe Masalah\n",
        "Klasifikasi: Untuk masalah di mana tujuan utamanya adalah mengelompokkan data ke dalam kategori yang telah ditentukan, seperti Logistic Regression, Decision Trees, Random Forests, atau Support Vector Machines (SVM). Regresi: Untuk masalah di mana tujuan utamanya adalah memprediksi nilai numerik, seperti Linear Regression, Ridge Regression, atau Lasso. Clustering: Untuk mengelompokkan data yang tidak berlabel, seperti K-Means, DBSCAN, atau Hierarchical Clustering. Dimensionality Reduction: Untuk mengurangi jumlah fitur dalam data, seperti Principal Component Analysis (PCA) atau t-Distributed Stochastic Neighbor Embedding (t-SNE).\n",
        "\n",
        "### b. Ukuran dan Jenis Data\n",
        "Jumlah Data: Beberapa algoritma lebih cocok untuk dataset besar, seperti Neural Networks atau Gradient Boosting Machines. Algoritma lain seperti K-Nearest Neighbors (KNN) mungkin tidak efisien pada dataset yang sangat besar. Dimensionalitas: Data dengan banyak fitur mungkin memerlukan algoritma yang dapat menangani dimensi tinggi, seperti SVM dengan kernel trik atau Random Forest yang dapat menangani fitur yang tidak relevan. Data yang Hilang: Algoritma seperti Random Forest atau KNN dapat menangani data yang hilang lebih baik daripada algoritma lain seperti SVM.\n",
        "\n",
        "### c. Kompleksitas Model\n",
        "Model yang Sederhana: Algoritma seperti Linear Regression atau Logistic Regression mudah diinterpretasi dan diimplementasikan Model yang Kompleks: Algoritma seperti Neural Networks atau Gradient Boosting Machines dapat menangani hubungan yang lebih kompleks dalam data, tetapi sering kali lebih sulit diinterpretasi.\n",
        "\n",
        "### d. Waktu dan Sumber Daya Komputasi\n",
        "Training Time: Beberapa algoritma membutuhkan waktu pelatihan yang lama, seperti Neural Networks atau SVM dengan kernel yang kompleks. Inference Time: Untuk aplikasi yang membutuhkan prediksi cepat, seperti dalam sistem real-time, algoritma seperti Logistic Regression atau Decision Trees mungkin lebih sesuai. Ketersediaan Hardware: Algoritma seperti Neural Networks memerlukan hardware khusus seperti GPU untuk mempercepat training.\n",
        "\n",
        "### e. Overfitting dan Generalisasi\n",
        "Overfitting: Algoritma seperti Decision Trees atau Neural Networks rentan terhadap overfitting. Teknik seperti cross-validation, regularisasi, dan pruning dapat membantu mengurangi overfitting. Generalization: Algoritma seperti Random Forest atau SVM cenderung memiliki generalisasi yang baik jika di-tuning dengan benar.\n",
        "\n",
        "### f. Interpretabilitas Model\n",
        "High Interpretability: Algoritma seperti Linear Regression, Logistic Regression, dan Decision Trees mudah diinterpretasi dan dijelaskan kepada non-teknis. Low Interpretability: Algoritma seperti Neural Networks atau ensemble methods seperti Random Forests dan Gradient Boosting Machines lebih sulit diinterpretasi.\n",
        "\n",
        "### g. Sifat Data\n",
        "Linear vs Non-Linear: Untuk data yang memiliki hubungan linear, algoritma seperti Linear Regression atau SVM dengan kernel linear cocok. Untuk hubungan non-linear, Neural Networks atau SVM dengan kernel non-linear bisa lebih efektif. Outliers and Noise: Algoritma seperti Robust Regression atau Tree-based methods dapat lebih baik dalam mengatasi outliers dan noise dalam data.\n",
        "\n",
        "### h. Cross-validation and Hyperparameter Tuning\n",
        "Cross-Validation: Teknik seperti k-fold cross-validation digunakan untuk mengevaluasi performa model secara keseluruhan dan memastikan bahwa model tidak overfit. Hyperparameter Tuning: Algoritma seperti Grid Search atau Random Search digunakan untuk menemukan kombinasi hyperparameter yang optimal untuk model.\n",
        "\n",
        "### i. Use Case Spesifik\n",
        "Image Recognition: Convolutional Neural Networks (CNNs) sangat cocok untuk data gambar. Sequential Data: Recurrent Neural Networks (RNNs) atau Long Short-Term Memory (LSTM) cocok untuk data berurutan seperti teks atau data waktu.\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIbrLJg699ux"
      },
      "source": [
        "## 4. Teknik Evaluasi\n",
        "Teknik evaluasi dalam machine learning adalah langkah penting untuk menilai kinerja model dan memastikan bahwa model tersebut akan bekerja dengan baik pada data baru yang belum pernah dilihat sebelumnya. Berikut beberapa teknik evaluasi yang umum digunakan:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o--aHK199uy"
      },
      "source": [
        "### a. Train-Test Split\n",
        "Deskripsi: Memisahkan dataset menjadi dua bagian: data pelatihan dan data pengujian. Model dilatih pada data pelatihan dan dievaluasi pada data pengujian. Kelebihan: Sederhana dan cepat. Kekurangan: Hasil evaluasi bisa bergantung pada bagaimana data dipisahkan.\n",
        "\n",
        "### b. Cross-Validation\n",
        "#### K-Fold Cross-Validation\n",
        "Deskripsi: Membagi data menjadi k bagian (folds), melatih model pada k-1 folds, dan menguji pada 1 fold yang tersisa. Proses ini diulang k kali, setiap fold digunakan sebagai data pengujian sekali. Kelebihan: Mengurangi ketergantungan pada satu set data pengujian, memberikan estimasi kinerja yang lebih stabil. Kekurangan: Lebih memakan waktu dan sumber daya komputasi.\n",
        "\n",
        "#### Stratified K-Fold Cross-Validation\n",
        "Deskripsi: Mirip dengan k-fold, tetapi membagi data sehingga setiap fold memiliki distribusi kelas yang sama. Kelebihan: Lebih baik untuk dataset yang tidak seimbang.\n",
        "\n",
        "### c. Leave-One-Out Cross-Validation (LOOCV)\n",
        "Deskripsi: Setiap data poin digunakan sebagai data pengujian satu kali, sementara sisanya digunakan sebagai data pelatihan. Kelebihan: Menggunakan maksimal data untuk pelatihan. Kekurangan: Sangat memakan waktu dan sumber daya komputasi.\n",
        "\n",
        "### d. Bootstrap Sampling\n",
        "Deskripsi: Mengambil sampel dengan penggantian dari dataset untuk membuat beberapa set data pelatihan, dan menguji model pada data yang tidak termasuk dalam sampel. Kelebihan: Berguna untuk mengestimasi variabilitas model. Kekurangan: Dapat memerlukan banyak sampel untuk estimasi yang baik.\n",
        "\n",
        "### e. Holdout Method\n",
        "Deskripsi: Dataset dibagi menjadi tiga bagian: training set, validation set, dan test set. Model dilatih pada training set, di-tuning pada validation set, dan dievaluasi pada test set. Kelebihan: Memberikan cara untuk men-tune hyperparameter sebelum evaluasi akhir. Kekurangan: Membutuhkan lebih banyak data untuk membagi menjadi tiga set.\n",
        "\n",
        "### f. Time Series Cross-Validation\n",
        "Deskripsi: Teknik khusus untuk data time series di mana data dibagi berdasarkan waktu untuk memastikan bahwa data pengujian selalu lebih baru daripada data pelatihan. Kelebihan: Menghormati urutan temporal data. Kekurangan: Bisa lebih rumit untuk diterapkan.\n",
        "\n",
        "### g. Hyperparameter Tuning dan Model Selection\n",
        "Grid Search: Mencari kombinasi hyperparameter terbaik dengan mencoba setiap kemungkinan kombinasi. Random Search: Mencari kombinasi hyperparameter terbaik dengan mencoba kombinasi secara acak. Bayesian Optimization: Mencari kombinasi hyperparameter terbaik dengan pendekatan probabilistik.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm9xXRqg99uz"
      },
      "source": [
        "## 5. Optimasi Parameter\n",
        "Optimasi parameter (juga dikenal sebagai tuning hyperparameter) adalah proses penting dalam machine learning yang bertujuan untuk menemukan set parameter yang optimal untuk model sehingga kinerjanya dapat ditingkatkan. Berikut adalah beberapa metode umum yang digunakan untuk optimasi parameter:\n",
        "\n",
        "1. Grid Search: Mencari kombinasi optimal dari hyperparameter dengan mencoba setiap kombinasi dalam ruang parameter yang telah ditentukan.\n",
        "2. Random Search: Memilih kombinasi hyperparameter secara acak dalam ruang pencarian yang telah ditentukan.\n",
        "3. Bayesian Optimization: Menggunakan metode probabilistik untuk memilih kombinasi hyperparameter berdasarkan kinerja sebelumnya.\n",
        "4. Hyperband: Metode optimasi hyperparameter yang efisien dengan menggunakan teknik early-stopping.\n",
        "5. genetic Algorithms: Menggunakan prinsip evolusi dan seleksi alam untuk mencari kombinasi hyperparameter yang optimal.\n",
        "6. Simulated Annealing: Menggunakan prinsip fisika dari annealing untuk menemukan kombinasi hyperparameter yang optimal.\n",
        "7. Tree-structured Parzen Estimator (TPE): Menggunakan model probabilistik berbasis pohon untuk mencari kombinasi hyperparameter yang optimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Ayg06799u0"
      },
      "source": [
        "## 6. Dasar-dasar Deployment\n",
        "Deployment adalah proses mengeluarkan aplikasi atau sistem dari lingkungan pengembangan ke lingkungan produksi di mana aplikasi tersebut dapat diakses dan digunakan oleh pengguna akhir. Berikut adalah dasar-dasar yang perlu dipahami dalam proses deployment:\n",
        "\n",
        "### Persiapan Lingkungan\n",
        "Deployment machine learning melibatkan membawa model yang telah dilatih dan dioptimalkan ke dalam lingkungan produksi di mana ia dapat digunakan untuk membuat prediksi pada data baru. Proses ini mencakup beberapa langkah dan pertimbangan khusus. Berikut adalah langkah-langkah dasar dalam deployment machine learning:\n",
        "\n",
        "#### a. Persiapan Model\n",
        "Model Training: Melatih model menggunakan data pelatihan yang tersedia dan menyimpan model yang terlatih. Model Serialization: Menyimpan model yang telah dilatih dalam format yang dapat di-load kembali, seperti menggunakan pickle di Python atau format HDF5 untuk Keras/TensorFlow.\n",
        "\n",
        "#### b. Environment Setup\n",
        "Dependencies: Mengidentifikasi dan menginstal semua dependensi yang diperlukan untuk menjalankan model, termasuk library machine learning (misalnya, TensorFlow, PyTorch), preprocessing tools (misalnya, pandas, scikit-learn), dan runtime (misalnya, Python). Virtual Environments: Menggunakan virtual environments atau container (misalnya, Docker) untuk mengisolasi dan mengelola dependensi.\n",
        "\n",
        "#### c. Model Serving\n",
        "REST APIs: Menyediakan model sebagai layanan web dengan API menggunakan framework seperti Flask, FastAPI, atau Django. Ini memungkinkan aplikasi lain untuk mengirim data dan menerima prediksi melalui permintaan HTTP. Model Server: Menggunakan alat khusus untuk serving model seperti TensorFlow Serving, TorchServe, atau MLflow. Alat ini dirancang untuk mengelola dan mengoptimalkan serving model machine learning. Batch Processing: Untuk prediksi dalam jumlah besar, batch processing dapat digunakan, di mana data dikumpulkan dan diproses sekaligus, misalnya menggunakan Apache Spark.\n",
        "\n",
        "#### d. Scalability and Performance\n",
        "Horizontal Scaling: Menambah lebih banyak instance server untuk menangani beban yang meningkat. Load Balancing: Menggunakan load balancer untuk mendistribusikan lalu lintas ke berbagai instance server secara efisien. Caching: Mengimplementasikan caching untuk menyimpan hasil prediksi yang sering diminta untuk mengurangi beban komputasi.\n",
        "\n",
        "#### e. Monitoring and Logging\n",
        "Model Monitoring: Mengawasi kinerja model di produksi untuk mendeteksi degradasi performa, bias, atau drift data. Alat seperti Prometheus dan Grafana dapat digunakan untuk monitoring. Logging: Menyimpan log prediksi, kesalahan, dan metrik kinerja untuk analisis dan pemecahan masalah. Alat seperti ELK Stack (Elasticsearch, Logstash, Kibana) atau Splunk sering digunakan.\n",
        "\n",
        "#### f. Security and Compliance\n",
        "Data Privacy: Memastikan data pengguna dilindungi dan sesuai dengan regulasi seperti GDPR. Authentication and Authorization: Mengamankan API dengan mekanisme otentikasi dan otorisasi seperti OAuth. Model Security: Melindungi model dari serangan seperti model inversion dan membership inference.\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYrP4Pfz99u0"
      },
      "source": [
        "### Contoh Deployment dengan Flask dan Docker\n",
        "Berikut adalah contoh sederhana tentang bagaimana meng-deploy model machine learning menggunakan Flask dan Docker:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from flask import Flask, request, jsonify\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler # Untuk scaling data\n",
        "\n",
        "# --- Bagian 1: Melatih dan Menyimpan Model ---\n",
        "# Muat dataset contoh (Ganti dengan dataset Anda)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Scaling data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Latih model (Ganti dengan model yang Anda inginkan)\n",
        "model = LogisticRegression(max_iter=1000) # Meningkatkan max_iter\n",
        "model.fit(X, y)\n",
        "\n",
        "# Simpan model dan scaler ke file\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "print(\"Model dan scaler telah disimpan.\")\n",
        "\n",
        "# --- Bagian 2: Deployment dengan Flask ---\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Muat model dan scaler\n",
        "with open('model.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "with open('scaler.pkl', 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    try:\n",
        "        data = request.get_json(force=True)\n",
        "        features = data['features']\n",
        "        # Scaling data input\n",
        "        scaled_features = scaler.transform([features])\n",
        "        prediction = model.predict(scaled_features)\n",
        "        return jsonify({'prediction': prediction[0]})\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500 # Mengembalikan error 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Untuk deployment produksi, gunakan server WSGI seperti Gunicorn\n",
        "    # app.run(host='0.0.0.0', port=5000)\n",
        "    print(\"Jalankan dengan server WSGI untuk deployment produksi.\")"
      ],
      "metadata": {
        "id": "0aojBeNeCWzD",
        "outputId": "9c12da2b-7bde-49e5-c938-2b003f2fdca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model dan scaler telah disimpan.\n",
            "Jalankan dengan server WSGI untuk deployment produksi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gunicorn"
      ],
      "metadata": {
        "id": "Azsd_c7JCeAA",
        "outputId": "93042756-ca7e-45e6-a21f-9bd468c91e14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gunicorn\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gunicorn) (24.2)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[?25l   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m0.0/85.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mââââââââââââââââââââââââââââââââââââââ\u001b[0m\u001b[91mâ¸\u001b[0m\u001b[90mâ\u001b[0m \u001b[32m81.9/85.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gunicorn\n",
            "Successfully installed gunicorn-23.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gunicorn --workers 4 --bind 0.0.0.0:5000 main:app"
      ],
      "metadata": {
        "id": "IcBdzwtFChoB",
        "outputId": "788b63f6-7299-4a86-d178-285650a423ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-14 04:25:25 +0000] [5049] [INFO] Starting gunicorn 23.0.0\n",
            "[2025-05-14 04:25:25 +0000] [5049] [INFO] Listening at: http://0.0.0.0:5000 (5049)\n",
            "[2025-05-14 04:25:25 +0000] [5049] [INFO] Using worker: sync\n",
            "[2025-05-14 04:25:25 +0000] [5050] [INFO] Booting worker with pid: 5050\n",
            "[2025-05-14 04:25:25 +0000] [5050] [ERROR] Exception in worker process\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/arbiter.py\", line 608, in spawn_worker\n",
            "    worker.init_process()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/workers/base.py\", line 135, in init_process\n",
            "    self.load_wsgi()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/workers/base.py\", line 147, in load_wsgi\n",
            "    self.wsgi = self.app.wsgi()\n",
            "                ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n",
            "    self.callable = self.load()\n",
            "                    ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/app/wsgiapp.py\", line 57, in load\n",
            "    return self.load_wsgiapp()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/app/wsgiapp.py\", line 47, in load_wsgiapp\n",
            "    return util.import_app(self.app_uri)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/util.py\", line 370, in import_app\n",
            "    mod = importlib.import_module(module)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1140, in _find_and_load_unlocked\n",
            "ModuleNotFoundError: No module named 'main'\n",
            "[2025-05-14 04:25:25 +0000] [5050] [INFO] Worker exiting (pid: 5050)\n",
            "[2025-05-14 04:25:25 +0000] [5051] [INFO] Booting worker with pid: 5051\n",
            "[2025-05-14 04:25:25 +0000] [5051] [ERROR] Exception in worker process\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/arbiter.py\", line 608, in spawn_worker\n",
            "    worker.init_process()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/workers/base.py\", line 135, in init_process\n",
            "    self.load_wsgi()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/workers/base.py\", line 147, in load_wsgi\n",
            "    self.wsgi = self.app.wsgi()\n",
            "                ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n",
            "    self.callable = self.load()\n",
            "                    ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/app/wsgiapp.py\", line 57, in load\n",
            "    return self.load_wsgiapp()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/app/wsgiapp.py\", line 47, in load_wsgiapp\n",
            "    return util.import_app(self.app_uri)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/util.py\", line 370, in import_app\n",
            "    mod = importlib.import_module(module)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1140, in _find_and_load_unlocked\n",
            "ModuleNotFoundError: No module named 'main'\n",
            "[2025-05-14 04:25:25 +0000] [5051] [INFO] Worker exiting (pid: 5051)\n",
            "[2025-05-14 04:25:25 +0000] [5052] [INFO] Booting worker with pid: 5052\n",
            "[2025-05-14 04:25:26 +0000] [5049] [ERROR] Worker (pid:5050) exited with code 3\n",
            "[2025-05-14 04:25:25 +0000] [5052] [ERROR] Exception in worker process\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/arbiter.py\", line 608, in spawn_worker\n",
            "    worker.init_process()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/workers/base.py\", line 135, in init_process\n",
            "    self.load_wsgi()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/workers/base.py\", line 147, in load_wsgi\n",
            "    self.wsgi = self.app.wsgi()\n",
            "                ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n",
            "    self.callable = self.load()\n",
            "                    ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/app/wsgiapp.py\", line 57, in load\n",
            "    return self.load_wsgiapp()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/app/wsgiapp.py\", line 47, in load_wsgiapp\n",
            "    return util.import_app(self.app_uri)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/util.py\", line 370, in import_app\n",
            "    mod = importlib.import_module(module)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1140, in _find_and_load_unlocked\n",
            "ModuleNotFoundError: No module named 'main'\n",
            "[2025-05-14 04:25:26 +0000] [5052] [INFO] Worker exiting (pid: 5052)\n",
            "[2025-05-14 04:25:26 +0000] [5049] [ERROR] Worker (pid:5051) was sent SIGTERM!\n",
            "[2025-05-14 04:25:26 +0000] [5049] [ERROR] Worker (pid:5052) exited with code 3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/arbiter.py\", line 201, in run\n",
            "    self.manage_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/arbiter.py\", line 570, in manage_workers\n",
            "    self.spawn_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/arbiter.py\", line 642, in spawn_workers\n",
            "    time.sleep(0.1 * random.random())\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/arbiter.py\", line 241, in handle_chld\n",
            "    self.reap_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/arbiter.py\", line 529, in reap_workers\n",
            "    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\n",
            "gunicorn.errors.HaltServer: <HaltServer 'Worker failed to boot.' 3>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/gunicorn\", line 8, in <module>\n",
            "    sys.exit(run())\n",
            "             ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/app/wsgiapp.py\", line 66, in run\n",
            "    WSGIApplication(\"%(prog)s [OPTIONS] [APP_MODULE]\", prog=prog).run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/app/base.py\", line 235, in run\n",
            "    super().run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/app/base.py\", line 71, in run\n",
            "    Arbiter(self).run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/arbiter.py\", line 228, in run\n",
            "    self.halt(reason=inst.reason, exit_status=inst.exit_status)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/arbiter.py\", line 341, in halt\n",
            "    self.stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/arbiter.py\", line 395, in stop\n",
            "    time.sleep(0.1)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/arbiter.py\", line 241, in handle_chld\n",
            "    self.reap_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gunicorn/arbiter.py\", line 529, in reap_workers\n",
            "    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\n",
            "gunicorn.errors.HaltServer: <HaltServer 'Worker failed to boot.' 3>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWx_RFXi99u3"
      },
      "source": [
        "Deployment machine learning mencakup berbagai aspek teknis dan non-teknis, mulai dari persiapan model hingga monitoring di produksi. Dengan pemahaman dan penerapan yang tepat dari konsep-konsep ini, proses deployment dapat dilakukan secara efisien dan aman."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}